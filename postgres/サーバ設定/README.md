# サーバ設定

サーバのリソースを効率的に利用するために必要となるOSのパラメータ設定(CPU/メモリ/ディスクIO)について

## CPUの設定

### クライアント接続設定

- PostgreSQLはクライアントからの接続要求ごとにバックエンドプロセスが1つ作成され、トランザクションや問い合わせを処理する
- バックエンドプロセスへのCPU割り当てはカーネルがスケジューリングするため、CPUコア数より接続数が多くても問題ないが、数が多すぎるとプロセスのコンテキストスイッチの切り替えが頻繁に発生するため、データベースの用途や利用者の数を踏まえて適切な値を設定する
- クライアント接続はpostgreql.confで設定する。設定を変更した場合、DBの再起動が必要
- クライアント接続設定の注意点
    - スタンバイサーバを運用している場合、スタンバイのmax_connectionsの設定をプライマリと同じか、それ以上に設定しておく必要がある。プライマリとスタンバイは設定ファイルが別々になり、プライマリだけ接続数の設定を変えてしまうと接続数が足りなくなり、スタンバイは起動できなくなってしまう。

### ロックの設定

- CPU処理に関わる設定にデッドロック検出がある。デッドロックを検出するにはDBに負担がかかるため、頻繁に起こらないように猶予時間(deadlock_timeout)が設定されている。デフォルト値は1000ミリ秒(1秒)
- 大量のトランザクションによってロック待ちが頻発するような場合は、デッドロック検出処理そのものが性能低下の原因になるためデフォルト値より大きめに設定することが推奨される

| パラメータ名 | デフォルト値 | 説明 |
| --- | --- | --- |
| max_connections | 100 | データベースの最大同時接続数 |
| superuser_reserved_connections | 3 | PostgreSQLのスーパーユーザ用に予約する接続数 |

## メモリの設定

- PostgreSQLはデータをWALとデータベースファイルとしてディスクに保存することでデータの永続化を実現しているが、HDDなどからデータを取り出す時間と、メモリからデータを取り出す時間は、数百倍~数十万倍の性能差がある
- メモリを活かすため、データアクセス時にデータベースファイルをページ単位でメモリ上に展開し、繰り返しデータにアクセスする場合の処理性能を高めている。
    - データベースの性能を高めるにはメモリの活用は非常に重要で、そのためにOSのメモリ設定とPostgreSQLのメモリ設定は適切にしておくべき

### OSのメモリ設定

- OSは共用メモリの最大容量に制限を設けている。Linuxにおいて制限を受ける可能性のあるカーネルパラメータは以下
    - 共有メモリセグメントの最大容量を制限する「shmmax」
    - 使用可能な共有メモリの総量を制限する「shmall」
- カーネルパラメータの初期値はOSのディストリビューションによって異なり、Linuxでは次のコマンドで設定値を変更できる

    ```bash
    $ sysctl -w kernel.shmmax=17179869184
    $ sysctl -w kernel.shmall=4194304
    ```

- なお、kernel.shmallはサーバ全体で利用可能な共有メモリの上限となるため、PostgreSQLだけで16GBの共有メモリを取得することはできない。sysctlで設定した値はサーバを再起動するとデフォルト値に戻ってしまうため、/etc/sysctl.confファイルに設定値を保存することを強く推奨

### PostgreSQLのメモリ設定

- 共有メモリ領域の設定
    - PostgreSQLのメモリ設定の中でとくに重要なパラメータは「shared_buffers」
        - POstgreSQLが共有バッファのために確保する共有メモリのサイズを設を定する
        - 初期値は128MBと比較的小さな値が設定されているため、ほとんどの場合で設定値を変更することが推奨されている
        - 目安はメモリを1GB以上搭載したサーバであれば、その25%程度を設定するとよい。
        - PostgreSQLでは共有バッファを使い切ると利用されていないページをバッファから追い出すため、追い出されたデータを再読み込みする場合は処理性能が落ちる。しかし共有バッファが大きいほど性能が良いというわけでなく、サイズが大きくなるとバッファ探索に時間がかかるようになるほか、データベースファイルに書き出すチェックポイント処理の負担も大きくなる。そのため適度な大きさの共有バッファを設定することが推奨
        - 仮にshared_buffersから追い出されても、追い出された直後のデータはOSのディスクキャッシュに残っている可能性があるため、バッファへの再読み込みは比較的高速であることも、共有バッファを大きくしすぎない根拠となる
    - PostgreSQL14からはパラレルクエリ用にあらかじめ共有メモリの確保を行うパラメータmin_dynamic_shared_memoryが作成された
        - パラレルクエリを多用するユースケースでは共有メモリをあらかじめ確保しておくことで都度、動的にメモリ確保するオーバーヘッドの軽減が期待される
    - そのほかの共有メモリ領域のパラメータは通常、デフォルト設定でシステム要件に必要なだけ確保する値になっているため、共有バッファのような性能を意識したチューニングは不要
    - なお共有メモリはPostgreSQLの起動時に確保されるため、設定値の変更にはPostgreSQLの再起動が必要
- プロセスメモリ領域とその設定について
    - プロセスメモリ領域の設定は、プロセス単位でメモリを確保するため、設定値よりかなり大きなメモリを消費することに注意が必要
    - 「work_mem」を大きくするとメモリ上でソートやハッシュ操作ができるため問い合わせの性能は向上するが、複雑な問い合わせの場合にはソートやハッシュ操作が問い合わせの中で複数回実行されることがある。その場合work_memのサイズの数倍のメモリが必要になる。メモリ不足からスワップが発生してしまうとかえって性能が悪くなる
    - 「maintenance_work_mem」はメンテナンス操作時に一時的に大きな値を設定することで、手動バキュームやインデックス作成、外部キー作成などが高速になる。デフォルト設定では自動バキュームでも同時実行数actovacuum_max_workers×maintenance_work_memのメモリを消費する。
    - 「autovacuum_work_mem」は「maintenance_work_mem」設定時に自動バキュームが実行されても影響を受けないよう、自動バキューム時のメモリ利用量を設定するパラメータ

### HugePage設定

- PostgreSQLでは共有メモリを大きくすることでDBの性能向上を図るが、メモリ管理に用いるページテーブルも肥大化しCPU負荷が増加してしまい、性能にも影響がでる
- LinuxではHugePage機能を使うことによってページテーブルを小さくでき、性能低下を抑えることが期待できる。PostgreSQLでHugePage機能を利用するためには、OSで設定した後にpostgresql.confを設定する必要がある。
    - HugePage数はPostgreSQLのpostmasterプロセスのVMPeakの値から算出する。sysctlで設定した値はサーバを再起動するとデフォルト値に戻ってしまうため/etc/sysctl.confファイルに設定値を保存することが強く推奨されている
- PostgreSQLがHugePage機能を利用するか否かはpostgresql.confのパラメータ(huge_pages)によって決定する。huge_pagesはon/off/tryのいずれかを設定可能(デフォルトはtry。HugePage機能の利用を試みて成功した場合はHugePage機能を利用するという設定)
- HugePage数の算出と設定

```bash
・PostgreSQLを起動してpostmasterのプロセス番号を取得する
$ head -1 $PGDATA/postmaster.pid
10842

・プロセス番号からVmPeakの値を取得する
# grep ^VmPeak /proc/10842/status
VmPeak: 8856980 kB

・HugePageサイズを取得する
# grep ^Hugepagesize /proc/meminfo
Hugepagesize: 2048kB

・HugePageサイズとVmPeakの値からPostgreSQLが必要とするHugePage数を算出する
8856980 / 2048 = 4324.6(= 4330)

・HugePage数を設定する
# sysctl -w vm.nr_hugepages=4330
```

## ディスクの設定

- 一般的なデータベースシステムはディスク性能がシステムのボトルネックになりやすい
- ボトルネックにしないための設定は現実的なコストでは実現が困難なため、OSとPostgreSQLの標準設定よりも効率よくディスク性能を発揮できるように設定することが重要

### OSのディスク設定

- I/Oスケジューラー
    - I/Oスケジューラーの初期設定は以下コマンドで確認可能

    ```bash
    $ cat /sys/block/sda/queue/scheduler

    # デバイスsdaをnoneに設定する場合
    $ echo none > /sys/block/sda/queue/scheduler
    # 上記は再起動すると戻るため永続的に変更する場合grub2-mkconfigコマンドで反映する
    # cat /etc/default/grub

    GRUB_CMDLINE_LINUX="console=ttyS0,...略... elevator=none"
    # grub2-mkconfig -o /boot/grub/grub.conf
    ```

    - 以下I/Oスケジューラの設定値

    | 設定項目 | 説明 |
    | --- | --- |
    | none(もしくはnoop) | OSはスケジュールに関与しない |
    | anticipatory | I/O要求に対してHDDドライブの中の物理的な配置に近いデータを優先して処理する |
    | md-deadline(deadline) | I/O要求の待ち時間に限界値(deadline)を設け、限界に近いI/O要求を優先して処理する |
    | bfq(cfq) | I/O要求が特定のプロセスやアプリケーションに占有されないように時間や帯域を分散する |
    | kyber | read/writeのレイテンシをそれぞれの目標値に近づけるように調整する |
    - PostgreSQLではデータ書き込みプロセス(bgwriter)やWAL書き込みプロセス(wal writer)といった小数のプロセスがI/O要求の大半を占め、データアクセスもランダムアクセスが多いために、mq-deadlineに設定することが推奨されている
- ディスクのシステム上に関するpostgresql.confのパラメーター(エラーが発生しない限り初期状態から変更する必要はない)

| パラメータ名 | デフォルト値 | 説明 |
| --- | --- | --- |
| temp_file_limit | -1 | あるプロセスが一時ファイルとして利用可能なディスクの最大容量 |
| max_files_per_process | 1000 | あるプロセスが同時に開くことのできるファイル数の上限 |
- バックグラウンドライタに関するpostgresql.confパラメータ

| パラメータ名 | デフォルト値 | 説明 |
| --- | --- | --- |
| bgwriter_delay | 200ms | バックグラウンドライタの動作周期。動作周期の現実的な最小粒度は多くの場合10msであるため、10ms未満の粒度で設定変更しても動作周期は切り上がる |
| bgwriter_Iru_maxpages | 100 | 一度にバックグラウンドが書き込むページ数の上限 |
| bgwriter_Iru_multiplier | 2.0 | 書き込みが必要になったページのうち、どのくらいの割合を書き込むかの計算に利用 |
| bgwriter_flush_after | 512kB(Linux時)/
0(それ以外) | バックグラウンドライタの書き込みが設定値を超えた場合に、OSに対しても記憶媒体への書き込みを強制する。単位省略時はページ数の指定となり、設定値0はOSに対して書き込みの強制を無効化する |
- WALに関するpostgresql.confパラメータ

| パラメータ名 | デフォルト値 | 説明 |
| --- | --- | --- |
| walwriter_delay | 200ms | WALライタの動作周期。動作周期の現実的な最小粒度は多くの場合10msであるため、10ms未満の設定変更は効果が現れないことがある |
| max_wal_size | 1GB | チェックポイントの間にWALが増加する最大サイズ。最後のチェックポイント実行からこのサイズのWALが生成されるとチェックポイント処理が動作する |
| min_wal_size | 80MB | リサイクル対象となる古いWALファイルのサイズ。チェックポイント後に設定値のファイルサイズ分は削除されず再利用可能な状態で維持される |
| checkpoint_timeout | 5min | チェックポイントの間隔。最後のチェックポイント実行からこの時間が経過するとバッファ上のデータをディスクに書き出すチェックポイント処理が動作する |
| checkpoint_completion_target | 0.9 | 次のチェックポイント発生までのインターバルのうち、チェックポイント完了までの時間の比率 |
