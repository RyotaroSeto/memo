# microservices-memo
## マイクロサービスのメリット

- 保守性が上がる(テストの範囲が責苦なる)
- 各サービスごとに分けられるため、テスト時間、ビルド時間が大幅に短縮される
    - 大規模なモノリスアプリだと、アプリ全体のビルドに数時間単位かかる
- ビジネスの迅速性や俊敏性が求められているため、加速できる(1日に何回もリリースできるため)
- サービスの復旧時間向上
- 変更時の障害率低下

## マイクロサービスのデメリット

- メンテナンス性の複雑さ
- 小さなアプリならモノリスの方がよい
- チームのスキル感(低めだとやめといた方がよい)

echo 'export PATH="'$PWD'/bin:$PATH"' >> ~/.bash_profile

## マイクロサービスとは

マイクロサービスとはただサービスごとに分割する狭義のマイクロサービスではなく、

以下のようにビジネスの俊敏性を高める手段である。

- アジャイル開発でより顧客への価値提供スピードの重視
- 開発のアジリティを上げるためにインフラレイヤの柔軟性の向上
- DevOpsによるCI/CDによるビルドリリースプロセスの自動化で人手の極小化

## gRPCメリット

- 通信データをバイナリ形式にすることによって、JSON形式よりも通信量を軽減し、さらにHTTP2ベースでやりとりすることによって通信コストを最大限削減している
- Protocol Buffersを使って、バイナリ形式にシリアライズされるため、JSONやXMLに比べて非常に小さいデータサイズとなり、効率的な通信が可能
- バイナリベースのメッセージ通信やHTTP2通信により、低レイテンシ(情報の送信から受信までの遅延が非常に短いこと)や高スループット(単位時間あたりに処理できるデータやトランザクションの量が多いこと)が可能
- 双方向ストリーム通信が可能

## gRPCデメリット

- ブラウザから呼び出すサポート機能が限定的なため、ブラウザから直接gRPCを呼ぶことは難しい
- HTTP2通信なため、インフラで途中HTTP2非対応の機器があると問題が発生する場合あり

## BFFが必要になった背景

- クライアント端末の種類の増加と、それに伴うロジックの増加が要因
    - web、スマホweb、スマホアプリ、デスクトップアプリなど複数のクライアントが登場し、それぞれにメッセージやコンテンツの出し訳を実装しようとすると、フロントエンド側のコードが複雑化し、冗長なコードになるため。その部分を切り出した

## GraphQL

- サーバー側を変更せずにクライアント側で必要な情報を柔軟に決定できるためBFFの実装方法として有力
- HTTP1.1

## GraphQLのメリット

- 通信効率向上
    - クライアントが必要なデータのみ取得できるため、ネットワーク負荷やレスポンス時間削減できる
- 型付けのAPI定義
- 柔軟性向上
    - 新たなユースケースに伴うクエリの追加変更やデータソースの追加変更にサーバー側で柔軟に対応できる

## GraphQLのデメリット

- キャッシュの難易度が高い
    - 単一のエンドポイントで受け付けるため、HTTPのCache-Control Headerを利用したキャッシュ戦略を通常利用できない。パフォーマンスの劣化を防ぐため、GraphQL用のキャッシュ対策が必要
- セキュリティ対策
    - クエリの柔軟性が高いため、不正なクエリを防ぐにはサーバ側で適切なバリデーションが必要
- ファイルアップロード
    - ファイルアップロードに対応できていないため、別エンドポイントにファイルアップロード用のAPIを実装する必要あり
- エラーハンドリング
    - アプリケーション層のプロトコルのため、エラーが発生しても通常はHTTPステータス200が返ってくる。クライアントはレスポンス内容を見てエラーハンドリングする必要ある

## RESTとの比較

- GraphQLを選ぶべき場合
    - 利用ユースケースが多く、ユースケースごとの実装効率を重視したい場合
    - 複雑なクエリやネストされたデータが多く、効率よくデータ取得したい
- Restを選ぶべき場合
    - 実装のシンプルさ
    - 既存を維持

## ****なぜProbeが必要か****

- アプリケーションがコンテナの起動からトラフィックを受け付けられるようになるまで時間がかかる場合、まだトラフィックを受け付けられないのにトラフィックは流れてくるので、500エラーを返してしまうことになる。
    - ういったことが起きないようにProbeを設定することで、いつトラフィックを流してよいのか、いつコンテナアプリケーションが起動しているのか、いつPodを再起動すればよいか、をKubernetesに指示できる

## ****Probeの種類****

- Liveness Probe
    - kubeletはコンテナの状態を定期的に確認し、コンテナが異常な状態であればPodを再起動する
    - 異常な状態のPodを再起動するため使用
    - Liveness Probeを設定しないと、アプリケーションは異常な状態なのにKubernetesはそれを正常な状態だとみなし、Podを再起動することなく起動し続けるといったことが起こりえる
- Readiness Probe
    - コンテナがトラフィックを受け入れられる状態であるかkubeletに伝える
    - トラフィックが受け入れられないPodをServiceのロードバランシングから切り離す
    - **挙動で重要なポイント**
        - **コンテナの全てのライフサイクルにおいて実行される**こと
        - Podが起動したときだけではなく、Podが起動してる間ずっと繰り返しProbeを実行し続ける(アプリケーションが一時的に使えない場合を検知するため)これにより、アプリケーションを再起動せずに、アプリケーションが再び使えるようになるまでトラフィックを止める。アプリケーションが使える状態になったら、トラフィックを再び送りはじめる
- Startup Probe
    - Podの起動時にのみ実行されるReadiness Probe。
    - 起動に時間がかかるが、どれくらい時間がかかるか予測できないようなアプリケーションに対して有効
    - Readiness Probeでは `initialDelaySeconds` を設定することで、Readiness Probeを開始するまでの時間を設定することができる
    - `failureThresholdthat`の値を大きくすることで起動時間の変化に対応することもできますが、こちらを大きくすると実行中のアプリケーションが異常な状態になったことを検知するのにより多くの時間がかかることになってしまうので良くないです。
    - Startup Probeを定義することで、起動にかかる時間の変化に適切に対応することができる
    - 例えば、`failureThreshold`を`30`に、`periodSeconds`を`10`に設定すると、 30 * 10秒 = 300秒間はアプリケーションの起動を待ち続け、アプリケーションが素早く起動した場合はそのことを検知でき次第、Liveness Probeのチェックが開始される
    - コンテナアプリケーションの起動が完了したかを認識
    - Startup Probeが成功するまでは、Liveness Probeによるチェックを無効
        - v1.16でalpha、v1.18でbeta、v1.20でGA

****TCP****

- TCPによるProbeは、gRPCサーバーやFTPサーバーなどHTTP通信ができない場合に有効

```markdown
readinessProbe:
   tcpSocket:
     port: 21
```

**[ベストプラクティスも乗っている](https://zenn.dev/nekoshita/articles/4e838ae224ed56#%E3%83%99%E3%82%B9%E3%83%88%E3%83%97%E3%83%A9%E3%82%AF%E3%83%86%E3%82%A3%E3%82%B9)**

## サービスメッシュ

- サービスメッシュとは?
    - 横断的関心事(複数のマイクロサービスに共通的に利用される機能 ルーティング、サービスディスカバリ、セキュリティ、ロギング)をどう扱うか課題になるが、その解決の1つ
    - マイクロサービス間の通信を管理し、通信制御のための処理を挿入するための仕組み
    - 横断的関心事を個々のマイクロサービスのビジネスロジックから分離可能になる
    - 結果として各マイクロサービスからビジネスロジック以外の要素を極力排除し、シンプルになる

## ****Istio IngressGateway****

- istio ingressGatewayがトラフィックを受取、個々のServiceにルーティングさせる。ServiceはPodに転送。
- VirtualServiceリソースの設定を元に、istio ingressGatewayはリクエストをアプリケーションのServiceへ送る

## Istioを用いたトラフィック管理の重要要素

- VirtualService
    - サービスへのリクエストルーティングの設定を定義する。例えばHTTPヘッダやURLパスなどに基づいてルーティング先を決定したり、DestinationRuleと組み合わせてサービスのサブネット単位にルーティングルールを設定したりできる
- DestinationRule
    - サービスへのトラフィックに対するポリシーを定義する。サブセット(PodのLabelを用いたまとまり)を定義したり、負荷分散ルールを設定したり、最大接続数やタイムアウトなどの接続ポリシーを設定できる

## トラフィック管理に使用されるIstioのリソース

- Gateway
    - メッシュの境界に位置し、メッシュ外からのトラフィックを管理する。例えば、後悔するポートの設定やホスト単位のルーティング設定、HTTPS設定などできる
- Service Entry
    - メッシュ外のサービスを内部サービスレジストリに追加し、それらへの適切なルーティングを可能にする

## Istioによって実現するトラフィック制御の例

- ルーティング制御
    - 特定の条件に基づいてリクエストを異なるサービスにルーティング
    - 異なるバージョンのPod(アプリ)を同時にデプロイし、それらの間でルーティングを切り替えるデプロイ戦略の実現に活用できる
    - これにより「Blue-Greenデプロイ」や「カナリアリリース」などの実現可能
- トラフィック移行
    - 一部のトラフィックを新しいサービスに徐々にシフト
    - トラフィックの一部(例えば25%)を別バージョンに流すことも可能
- タイムアウト
    - サービスが応答するまでの最大時間の設定
    - アプリケーションで特別なタイムアウト処理を実装することなく、Istioが強制的にリクレスとを切断可能
    - Istioが強制的にコネクションを切断し、タイムアウトさせることができる
    - 特に微細な障害から全体的なパフォーマンスを最小限に抑えることが求められるマイクロサービスにおいて有用。
    - Istioではリクエストが失敗した場合にEnvoyプロキシがサービスへの**接続をリトライする最大回数を指定できる**
        - 各サービス単位だから細かなメソッドは流石に厳しい？
- サーキットブレイカー
    - サービスが**過負荷**の場合にその影響を他のサービスに伝播させない設定
        - 一定期間内にエラーが一定の閾値を超えると、自動的にリクエストを遮断
            - 待ち時間が長くなることなくエラーを早期に返せる
    - サービスが**障害状態**の場合にその影響を他のサービスに伝播させない設定
        - エラー率が一定の閾値を超えると、自動的にリクエストを遮断
            - 無駄なリクエストを防止できる
    - 依存関係のある他のサービスへの影響を最小限に抑えられる

## Istioを用いたセキュリティ

マイクロサービスはサービスごとにエンドポイントが公開されているため、不正なアクセスのリスクが高まる。また、ネットワーク経由で通信が行われるため、マイクロサービス間の通信が暗号化されていない場合、通信の傍受のリスクがある

## Istioのセキュリティ制御に関連するリソース

- Peer Authentication
    - マイクロサービス間の認証および通信の暗号化を実現するための、相互TLS設定を定義
- Request Authentication
    - リクエストの認証方法を定義。また外部アクセスユーザの認証に使用されるJWTの検証を定義することもできる
- Authentication Policy
    - サービスやユーザのアクセス権を設定するための認可機能を定義

## Istioによるクラスタ外部からのセキュアな通信

- IstioリソースのGatewayもセキュリティ管理に使用される。
- Gatewayでは、クラスタ外部からの通信の暗号化を実現するTLS終端を定義できる
    - 外部ネットワークにサービスを公開する場合に使用される

## Istioによるセキュリティ制御例

- 相違TLS認証(mTLS)
    - マイクロサービスの通信相手の正当性検証と、通信の暗号化を行い、マイクロサービス間通信の傍受や改ざんを防ぐ
- JWT検証
    - 独自に作り込むことなく認証機能を実現できる
    - JWT検証によるリクエストレベルの認証により、認証トークンを使ったエンドユーザ認証を実現
    - JWTを構成する要素
        - ヘッダ
            - トークンのタイプや署名アルゴリズムが含まれる
        - ペイロード
            - ユーザのIDやロールなどの情報が含まれる
        - 署名
            - 秘密鍵を使用して生成された署名が含まれる。受け取った側は公開鍵を利用して、トークンの正当性を確認
- アクセス制御
    - リクエスト元のサービスのアクセス権を制御する認可処理の実現

## 認証認可

- モノリスだと、セッションベースの認証が一般的。ユーザーの認証情報はサーバーサイドのセッションストアに保存されていた
- しかし、マイクロサービスは各サービスが独立して動作するため、セッション情報を共有するのが困難。マイクロサービスでセッションストアを利用した場合、セッションストアが単一障害点となり、各サービスが独立して動くマイクロサービスの利点が失われる
- マイクロサービス間の認証認可のプロトコルとしてOpenID Connect(**OIDC**)がある
    - OIDCは、IDプロバイダーから発行されるIDトークン(JWT形式)を用いてユーザー認証を行うことを標準化してもの。これにより、IDプロバイダーに対して一度認証を行うことで、その後のマイクロサービス間の認証認可を簡単に行える

## JWTの注意点

- 一度発行されたJWTの無効化が難しい。ユーザーがログアウトした場合でもすでに発行されたJWTはその有効期限が切れるまで有効なまま。
    - この問題を解決するためには、JWTの有効期限を短くする、またはJWTのブラックリストを作成するなどの対策が必要
- JWTに機密情報を含められない点。JWTの内容は署名により改ざんが防止されるが、その内容自体は暗号化されていない。
    - そのためJWTには最小限の情報のみを含めることが推奨される

## **Sagaパターンによるトランザクション管理**

- メリット
    - **柔軟性**:独立したサービス間で容易にトランザクションを行える。、各サービスが自身のビジネスロジックに集中することができ、拡張や変更が容易
    - **スケーラビリティ**:各サービスのローカルトランザクションが独立しているため、システム全体のスケーラビリティが向上。一部のサービスに負荷が集中しても他のサービスに影響を与えにくい
    - **障害回復**:補償トランザクションにより、障害が発生した場合でもシステムの一貫性を保つことができる
- デメリット
    - **複雑性**:補償トランザクションを設計・管理する必要があり、実装が複雑になる場合がある
    - **レイテンシの増加**:複数のローカルトランザクションを管理するため、ネットワーク通信が増加する。
- オーケストレーションベースのSAGA
    - オーケストレーターという中央でトランザクションを管理するコンポーネントが、次のステップに進むか否かを制御する方法。
- コレオグラフィベースのSAGA
    - 各サービスが次のステップに進むか否かを判断する方法。

## TCCパターンによるトランザクション管理

- 各サービスにそれぞれ要求。処理は完了とはせず、予約状態(tryフェーズ)として、全てエラーなく完了するか事前確認しOKだったら予約状態を確定する(Confirmフェーズ)。NGだったらCancelでtryフェーズでロックしたリソースを解放し、トランザクションを取り消す
- tryフェーズ

## イベントソーシングパターン

- イベントソーシングは簡単にいうとsqlのDeleteとアップデートを使わずINSERTだけして残しておく
- 銀行のシステムをはじめとした、エンティティに発生した出来事(口座からの引き出し、預け入れなど)をいつでも追跡可能であることが求められるシステムでイベントソーシングを採用することで恩恵を多く受ける
- メリット
    - アプリケーションで行った出来事の全てを記録して特定の時点のアプリケーション状態を復元できる点
- デメリット
    - 従来の永続化とは扱うデータが異なるため、求められる永続化のデータ形式も異なる

イベントソーシングサンプルコード
https://github.com/andreschaffer/event-sourcing-cqrs-examples

## APIコンポジション

- 複数のAPIを単一のAPIに統合する方法。
- メリット
    - 性能の向上
    フロントエンドから単一のAPIによってデータ取得ができ、複数のAPIにアクセスするよりも高速
    - 実装がシンプル
    
    APIコンポジションがあることで、フロントエンドから1度のリクエストで注文履歴を取得できる。
    

## CQRS

CQRSコマンド用とクエリ用に独立したデータベースを持つためスケーラビリティの観点も効果的
データの変更と取得の役割を明確に分離する

- Commands: システムの状態を変更するが、値を返さない(Void)
- Queries: 結果を返すが、システムの観測可能な状態を変化させない(副作用がない)
- Write と Read のデータソースを分離すると何らかの方法でデータの同期をとらないといけない
    - AWS RDS のリードレプリカ DB を使用できるならばその辺りの複雑さは考慮しなくて良くなる

CQRSとEvent Sourcingの相性が良い

## ポリシーチェック

- Keverno
- OPA

## 非機能テスト

- 可用性
    - 特定のサービスのダウンが全体的に波及しないようにサーキットブレイカーの考え方を取り入れる
    - 特定のサービスをあえてダウンさせるカオスエンジニアリングにより、運用体制を含めたシステムの回復性を検証する
    - KubernetesのReplicaSetにより、Podがダウンしてもオートヒーリング（自動復旧）される
- 性能、拡張性
    - パフォーマンステスト(K6)など
        - 単性能テスト
        - 限界性能テスト
        - 複合性能テスト
        - 連続運転テスト
            - システムが負荷を持続的に処理できるか確認(メモリやディスクなどのリソース使用量が増加し続け、パフォーマンスの低下やシステムの停止が起こらないか)
- 運用、保守性
    - 年末年始などの特異日において正常に動作するか
    - データのバックアップ方式や復旧に関わる考慮が十分にされているか
    - システム全体にわたってアプリケーションやハードウェアなどが運用監視できているか
- 移行性
    - 移行期間内に移行が完了するか
- セキュリティ
- 環境、エコロジー

## リリースとデプロイの違い

- リリース
    - ソフトウェア（OS,ミドルウェア、アプリなど）を利用可能な状態にすること
- デプロイ
    - ソフトウェアをテスト環境や本番環境に展開し、ユーザが利用できる状態にすること

## GoogleのFourKeysを計測する

- サービス復旧時間や変更のリードタイムなどい
- OSS(https://github.com/dora-team/fourkeys)
- https://devtron.ai/

## ステートレスにする

「ステートレス」は、システムやプログラムが外部の状態を保持せず、各処理が独立しているという性質を指す

## CI/CDの過去

従来はJenkinsがCI/CDまで一括して実施しており、このスタイルをCIOpsと呼ばれてrた

- CIOpsのデメリット
    - ツールが環境へのデプロイするための権限・クレデンシャルを保有し、強い権限を持つためのセキュリティ面にリスクがある。
    - 環境情報などのアプリコードもツールの中で書き換えられる可能性があり、ツールによるアウトプットの信頼性が不明瞭になるリスクある
    - パイプラインの実装が複雑になりやすく、メンテナンス性が低下するリスクある

## GitOpsによるCIOpsのデメリットの改善

- デプロイ先のクラスタ上にCDツールが配備されるので、クレデンシャルがクラスタ内で管理でき、セキュリティリスクをなくせる
- 命令型の定義ではなく状態の定義、すなわち宣言的に定義が可能なため、可読性が高く誰でも理解しやすい
- 開発者がデプロイ方法を定義できるようになり、属人化を排除できる

## Polyrepoで他のプロジェクト資産を活用したい場合

Javaの場合、Javaにおけるjarバイナリのようなアーティファクトを共有するためのアーティファクトリポジトリを利用する

## Monorepoメリットデメリット

- メリット
    - コードを共有しやすい
    - ツールの標準化が容易
    - コードを探しやすい
    - 状況を把握・管理しやすい
- デメリット
    - 共有しているコードの変更に伴う影響範囲が把握しにくい
    - マージ時の競合リスク
    - Monorepoに合うツール、権限等の制限が必要

## Polyrepoのメリットデメリット

- メリット
    - チームに権限委譲しやすい(強い独立性)
    - マージ時に競合が起こりにくい
    - マイクロサービス間の分離を強制しやすい
- デメリット
    - コーディング規約の運用が難しい
    - コードベースが分散して探しにくい
    - 規約・標準系の運用が難しい

## ブランチ戦略のユースケース比較

- Git Flow
    - 複雑なアプリケーションコードの管理が必要で、ある程度の期間をかけてリリースするようにするケース
- [GitHub Flow](https://gist.github.com/Gab-km/3705015#github-flow-1)
    - Git Flowをシンプルにしたブランチ戦略で、数日に複数回デプロイするようなケース
    - デメリットとして以下
        - 次のリリースでリグレッションの可能性ある
        - 以前のリリースに含まれていた変更が含まれていない可能性がある
- GitLab Flow
    - 環境ごとにアプリケーションコードを分離する複雑な管理が必要なケース
    - mainブランチを本番にデプロイできない場合、リリース専用ブランチを容易する
- トランクベース（筆者のおすすめ）
    - 短いイテレーションで開発を行い、迅速なフィードバックと修正が必要なケース
    - mainブランチでコードを共同作業するモデル
    - デプロイもmainのみ。
    - こうすることで、長期的存命なブランチを排除でき、マージ地獄も回避できる
    - 実現するためには以下の配慮が必要
        - 常にリリースできる準備を整え、ビルド・テストの異常を排除
        - 複数の開発中の機能がリリースされるので、フィーチャーフラグを使って機能をONOFFを制御できるようにする
        - 常にmainからリリースするように秩序を守る
        - 大規模なトランクベースはmainとfeature。また計画されたリリースの間に必要なバグ修正などしたい場合はreleaseブランチも作成可能。リリース頻度が高いチームではreleaseブランチを必要とせずmainからデプロイ。このようなケースではバージョンにコミット番号や日付などをつけることが多い。

## 効率的なパイプライン構築

- 細かいジョブに分割
    - パイプラインで実行するジョブを再利用可能な単位で分割する
    - 例えば、ビルド、テスト、静的チェックの処理をまとめて1つのジョブに定義するのでなく、それぞれの処理単位でジョブを定義する。
    - メリット
        - 複数のパイプラインで利用可能
        - デバッグが容易
        - パイプラインが途中でエラー終了しあ場合、エラーになったジョブから再実行でき復旧も迅速
        - 可読性の向上、修正時の影響範囲の把握のしやすさ・局所化によりメンテナンス性向上
        - 並列実行による高速化
- パイプラインの分離
    - CIパイプラインの実行時間の上限は10分未満が理想。
- 処理の高速化
    - キャッシュの利用
        - 一度Pullしたイメージを再利用する
        - ビルドキャッシュや外部から取得が必要なデータのキャッシュを利用
    - 並列化
    - サーバリソースの最適化およびスケーラビリティの確保
        - CIパイプラインの実行環境もサーバーなどのリソースをモニタリングし、リソースが不足している場合は必要なリソースを確保。また、スケーラブルにしておくことで、並列化による高速化や組織・チームのスケールやパフォーマンスが向上した際に追従可能
        - CIパイプラインのメトリクス計測
            - CIパイプラインの実行時間
            - ビルド時間
            - 開発者の待ち時間（リードタイム）
            - mainブランチのマージ失敗やCI失敗などの障害時間
            - CIパイプラインに関するツールのメンテナンス時間

## GitOpsの5つのベストプラクティス

- アプリケーションコード用とデプロイマニフェスト用のGitリポジトリを2つのリポジトリに分離する
- デプロイマニフェスト用のリポジトリの適切な数を選択する
- デプロイマニフェストをコミット・マージする前にテストする
- 外部変更により、Gitで管理しているデプロイマニフェストが影響を受けないようにする
    - Git上のmainやコンテナレジストリのlatestタグを参照すると、参照した時点と異なる状態となり得る。これにより意図しない不具合が混入する可能性があるため、常に特定の状態を指し示すバージョンやハッシュ値を指定する
- 機密情報の管理方法を計画する
    - 機密情報をGitで管理すると漏洩リスクが高い。KubernetesのExternal Sercret,Sealed Sercret、Vault 、Helm Secretなど使うと良い

## CDでの複数環境へのデプロイの考慮

- 環境ごとにクラスタを分離する
- コンテナイメージを再構築するのでなくプロモートする
- 環境情報・機密情報の管理をコンテナイメージから分離する
    - 環境変数はkubernetesであればConfigMapを利用
    - 機密情報はVaultsを利用
- 全ての環境に同じ方法でデプロイする
- 本番にデプロイするための唯一の手段とする

## デプロイとロールアウトの違い

デプロイはアプリケーションを環境に配備した状態

ロールアウトは、デプロイ後にサービスを利用者からアクセスできるようにした状態

## デプロイ戦略

- インプレース（再作成）デプロイ
    - 一気に総入れ替えする戦略
    - ダウンタイムが大きく、デプロイやロールアウト時に不具合が発生した際はロールバックを手作業で実施する必要がある。
    - 本番環境では極力利用せず、開発環境で変更の影響をなくすために再作成したいといった場合利用
- ローリングアップデート
- プログレッシブデリバリ
    - システムの影響を検証しながらリリースし、検証に失敗したら自動で切り戻すといったデリバリ手法。これにより障害が発生してもユーザーへの影響を限定的にし、かつ適切なタイミングでユーザーが利用できる
    - Blue-Greenデプロイ
        - GreenができたらBlueからGreenに一気に切り替える
        - ローリングアップデートと比較して瞬時にバージョンを切り替えられ、ロールバックも瞬時に可能。
        - Kubernetes標準の機能では実現できないため新しいツールの導入必要istioでできる
    - カナリアデプロイ
        - アプリの新バージョンを一部のユーザーに限定して公開し本番環境でテストを行うデリバリ戦略。
        - ダークカナリアデプロイは、実ユーザーではなく、システム開発者など内部のステークホルダーに限定する
    - feature Flag
        - ソフトウェアやサービスの機能をオンオフする機能フラグをアプリケーションに埋め込み、ユーザーにロールアウトするタイミングを制御する。Googleなど導入
        - カナリアデプロイはアプリケーションレベルのトラフィック制御だが、feature Flagは機能・UIなどより細かいレベルでロールアウト制御できる
        - アプリケーション面での実装が必要
    - A/Bテスト
        - feature Flagの拡張。AパターンとBパターンをユーザーに利用してもらい、収益がある方を採用するなど

## Argo Rolloutsとは

- Kubernetes向けのプログレッシブデリバリを提供する
- サービスの公開方法を制御し、メトリクスに基づいて成功や失敗時に自動プロモーションやロールバックを制御する
- Blue-Green,カナリアデプロイをサポートしている

## マイクロサービスでオブザーバビリティを使用する理由

下記を要約すると事象が起きた後でシステムにログインして情報を集めるといった方法が現実的でない。そのため後々解析に使うであろう情報は最初からシステムが外部に出力しておく。

- オートスケールによってインスタンスの数が自動的に増減する場合、スケールインによって自動的に終了され、調査したいと思った時にはすでにそのインスタンスが存在しない可能性がある
- 回復性にために障害が起きたコンテナの自動再起動などを導入すると、サービスの可用性を高められるが、調査したいと思った時にはすでに障害発生時の状態がホストから失われる可能性がある
- 複数サービスが強調し合うことでユーザからのリクエストを達成するために、分散したサービスのどこに性能のボトルネックがあるのか、個々のサービスを調査しても特定できない可能性がある
- 新しいサービスの追加やリクエストの量や内容の変化によって、サービス間の呼び出し関係は変動し複雑化していくため、調査したいと思った時にすでにそのトラフィックパターンが存在しない可能性がある

## オブザーバビリティと監視の関係

### 監視の定義

1. 監視対象の定義（どのホストか、どのようなデータ形式かなど）
2. 監視対象の正常値の定義（ある値がどのような範囲内であれば正常と判断するか）
3. 判定条件の定義（値が正常範囲を超えることをどの頻度・回数まで許容できるか）
4. 例外の定義（どのような条件であれば外れ値とみなして無視するか）
5. 異常時のアクションの定義（監視対象が正常でないと判定された時何をどうやって開発者に通知するか）

以下のようなものが監視にあたり、**事前にわかっているもの**である

- アプリケーションにエラーが発生したことを検知するために、ログに特定の文字列が含まれているか監視する
- ハードウェアの故障を検知するために、定期的なポーリングによる死活監視を行う

マイクロサービスは事前に予見できない**未知の障害が起こり得る可能性が高い**

監視はオブザーバビリティの一部にすぎない

## オブザーバビリティを実現するための取り組みとして監視以外

- 異なるシグナル同士を関連付けすることで、トラブルシューティングを効率化する
- サービスレベルを定義し測定することで、ユーザ体験を定量的に把握する
- サービスに意図的に障害を起こすことで信頼性に影響する未知の挙動を計測し、システムの回復性や運用プロセスを改善する
- CI/CDパイプラインやアプリケーションコードの品質を計測することでサービスの安定性を高める

## メトリクス

数値で表されるシグナル。「軌道しているコンテナ数」や「あるホストのCPU使用率」などメトリクスで表せられる。コンテナ数など値の変化の前後の差分が踏まれた背景情報を持たない分、データサイズが非常に小さい。監視対象が閾値を超えたことなどを契機にアラートで異常を伝える、キャパシティプランニングやインシデント予兆ととらえるために使用

## ログ

代表的な用途は以下

- 可視化
- 分析
- デバッグ
- 監査

## トレース

サービス間の呼び出しを記録したシグナル。

例えば、サービスAがエンドユーザからリクエストを受け取り、サービスBに必要な情報を参照し、結果をエンドユーザに返却した時、そのサービス間の呼び出し順序や所用時間といった情報をトレースで表せる。

分散トレーシングではトランザクションごとにIDを付与し、このIDをサービス間の呼び出しの際に引き渡していくことでサービスの呼び出し順序を記録する。

- 可視化
    - トレースIDごとのサービスの呼び出し順序や処理時間を時系列で把握するために利用
- 分析
    - トレースから得られる時系列の呼び出し関係の履歴を元に、リクエストの処理性能（レイテンシ）、サービス間の関係性・エラーの傾向など、統計的な洞察を得るために利用
    - サービスAはサービスBとサービスCのどちらを呼び出す割合が多いかなどの分析
- デバッグ
    - インシデント発生時のトランザクションの実行状況、とくにサービスのレイテンシに関しる問題特定で利用

## その他のシグナル

- プロファイル
    - 性能分析の目的で使用されるシグナル。
    - メモリリークの疑いのあるサービスが見つかった時、なぜメモリリークが起こっているのか、その根本原因を特定
    - コード中のどの命令がより多くのリソースを消費しているか、どの命令がレイテンシの遅延を発生させているかなどを特定する手段としてプロファイラが利用される
    - プロファイラはCPU、メモリヒープ、CPUやJVMプロファイラなどの言語固有のリソースも収集可能
    - データサイズが非常に大きいため、サンプリングなど保存するデータ量を低減する工夫も検討する
- ダンプ
    - プログラムの挙動を詳細に解析する目的で使用されるシグナル
    - あるプロセスが特定の条件でクラッシュする事象が見つかった場合、なぜクラッシュが起きてしまうのかを特定するためにメモリ状態を確認したいなど
    - ダンプはクラッシュ時のプロセスのメモリイメージをファイル出力することで取得できる
        - Linuxであればグローバル設定(/proc/sys/kernel/core_pattern)を使用して、コアダンプファイルをシステム内の任意の場所に書き込むように設定できる
    - データサイズが非常に大きい。
    - ダンプ収集はインフラストラクチャに強く依存するため、導入検討している場合以下注意
        - ホストのダンプ収集
            - サーバレスコンピューティングなどコンピューティングリソースへの特権的なアクセスが制限されている環境では、そもそもユーザに権限がなく取得できない場合あり
        - コンテナのダンプ収集
            - コンテナ内のエフェメラルボリュームにコアダンプファイルを出力すると再起動時に失われてしまうため、永続ボリュームなどを用いてコンテナ外へ保存する方法を検討する必要あり

## オブザーバビリティをシステムに導入するにあたり

- 最初に適用すべきスコープは、システムの中で最も信頼性に問題があるサービス群
    - 障害が起きた時に、その原因を特定できないことがよくある
    - 障害の原因特定に時間がかかりすぎている
    - ユーザの期待するサービスレベルを満たしていない、改善の見込みが立っていない
    - 原因を特定できない性能問題を抱えてる

## OpenTelmetry

- トレース、メトリクス、ログの収集に関する統一された使用と実装を提供する
- サービス間の呼び出し関係のトレース、システムの性能指標を収集する機能を提供する
- Otelを使ってアプリケーションの軽装を行っておけば、後からバックエンドの実装を比較的容易に変えられる
- 計装先のアプリケーションのシグナルを収集、変換、および送信するためにしようするOtelの重要なコンポーネントにOtel Collectorがある。Otelはシグナルを保存したり、可視化する機能は持たないため、収集したシグナルはこのCollectorによってさまざまなモニタリング、ロギング、トレーシングのツールと統合し、保存したり、可視化に利用する。
- Otel Collectorはデータを収集、変換、送信するプロキシサービスであり、データの収集、変換、フィルタリング、バッチ処理、エンリッチメント、ルーティングなど多くのユースケースをサポートしている
- Otel Collector内で動作するエクスポーターをプロバイダに提供することで、収集したトレース、メトリクス、ログデータを特定のバックエンドサービス（prometheus,datadogなど）に適合する形式に変換し、送信する役割を担う。

- 

## 優先して収集すべきメトリクスを選択するフレームワーク

- REDメソッド
    - サービスの提供品質を評価するためする基本的なもの。収集する指標に迷ってい場合は、REDメソッドから始めてみてもよい。Rate（リクエスト率）、Errors（エラー率）、Duration（レスポンス）の3つの要素で構成されている
- USEメソッド
    - リソースレベルでシステムの性能を評価するもの。REDメソッドで収集しているサービスの提供品質を保つために必要な情報を集めるなど保管的な役割。Utilizaton(利用率。リソースが使われている時間の割合)、Saturation(飽和度。リソースが上限まで使用され、さらに要求があった場合の待ち時間)、Errors（エラー数）の3つで構成されている
- Four Golden Signals
    - システムの性能と信頼性を評価するもの。Latency(レイテンシ。システムがタスクを完了するまでの時間)、 Traffic(トラフィック)、Errors（エラー数）、Saturation(飽和度)の4つで構成されている。

## メトリクスの収集方法

- フレームワークが自動的にメトリクスを公開する
    - 使用している開発フレームワークが自動的に一部のメトリクスを公開する
    - Spring Boot Actuatorのようなものはアプリケーションの稼働状況やHTTPリクエストの統計、データベースの接続情報など自動公開する。※開発者は特別なコードを書かなくて良い
- エージェントがメトリクスを収集し公開する
    - 特定のエージェントソフトウェアがアプリケーションからメトリクスを収集し公開。
    - Prometheusのnode_exporterなど
- アプリケーションが直接メトリクスを公開する
    - 開発者がアプリケーションコード内で明示的にメトリクスを定義し公開する
    - OpenTelemetryやPrometheusのクライアントライブラリで実装可能

## 構造化ログのメリット

- 分析しやすい
- メタデータ追加が用意
- インテグレーションしやすい

## ログの収集方法

- ノードごとにロギングエージェントを展開する
    - 各ノードにロギングエージェントをDaemonsetで配置し、ノード内で実行されている全てのコンテナのログを収集する。
- サイドカーコンテナを展開する
    - 各Podにロギングエージェントをサイドカーコンテナとして配置。アプリケーションコンテナはログを標準出力または標準エラーに書き出し、サイドカーコンテナがそれらの出力ストリームを読み取りログを収集する
- アプリケーションが直接ログを送信する
- **世の中の多くのログ管理ソリューションは、主にノードごとのロギングエージェントの展開によってログを収集している**

## コンテキスト伝播の設計

- 各サービスでリクエストやレスポンスを設計する際はコンテキストを適切に受け取り、次のリクエストに伝播するようの設計する必要がある
- 具体的に、HTTPリクエストヘッダやgRPCメタデータにトレースIDとスパンIDを含め、次のサービスがこれらを読み取って自身のスパンを生成するようにする。
- このようなトレースの計装は手動で行うことも可能だが、収集ツールでは自動計装機能も提供している。

## IstioのKiali

- マイクロサービス間の依存関係を表現したサービスマップを生成できる
- このマップは各サービスのヘルスステータスやトラフィック情報などがリアルタイムで反映される
- 例えば、サービスAがサービスBにリクエストを送り、その応答に時間がかかっている場合、その関係性が視覚化され、レイテンシなど確認できる

## マイクロサービスの9つの特徴

- サービスを通じたコンポーネント化
    - モノリスと異なり、コンポーネントはプロセスの境界を跨ぎ、API経由でアクセス
    - モノリスは同一プロセス内のコンポーネントを独立して構築したと思っても、コンポーネント間に思わぬ依存関係があったということは珍しくない
    - つまりプロセスを分けることは、コンポーネント間の依存関係を強制的に排除し、インターフェース設計(API設計)というタスクを明確に実施するための手段となる
    - モノリスは関数呼び出し、マイクロサービスはサービスをAPI呼び出し
- ビジネスケーパビリティに応じた組織
    - モノリスはフロント、バックエンド、基盤やデータモニタリングといった単位で組織を分割することが多いが、マイクロサービスはサービス単位で組織化される。
- プロジェクトではなくプロダクト
    - かつては開発チームがシステム開発プロジェクトを担い、リリースしたら運用チームに引き継ぐ開発スタイルが多かった。
    - マイクロサービスはプロジェクトが終わったら開発チームが解散するのでなく、プロダクトのライフサイクル全体にわたってサポートする。いわゆるDevOps
    - 運用やサポートに部分的にでも関わることで、開発者がプロダクトの振る舞いに日々接し、ユーザとの接点を増やせる。これによりソフトウェアの完成のみに注力するのでなく、ユーザのビジネスに貢献することにも注力しやすくなる
- スマートエンドポイントとシンプルな通信
    - これまではプロセス間通信を行う際、SOAPとESBの組み合わせで実現されたが、マイクロサービスでは、必要な機能はサービス（エンドポイント）に実装する
- 分散統治
    - 昔は中央集権的なガバナンスアプローチが当たり前で人材確保が高められるメリットがあったが、適材適所を阻害する原因にもなっていた
    - マイクロサービスでは分散型ガバナンスによって、各サービスを自律的に管理する。各サービスは独立して開発・運用でき、開発チームは自分たちの責任範囲内でサービスを設計運用できる
    - 例えば、ユーザ認証を担当するサービスと顧客データを管理するサービスでは、扱う情報や機能が異なるため、それぞれ最適な技術やツールを選択することが求められる
- 分散データ管理
    - モノリスではシステムで1つのデータベースが好まれる。
    - マイクロサービスでは、データ管理も分散され、各サービスが自身のデータ管理に責任を持つ。
- インフラの自動化
    - テスト自動化
        - 単体テスト
        - 機能テスト
        - パフォーマンステスト
    - デプロイの自動化
        - 開発環境、ステージング環境、本番環境へのデプロイを自動化
    - 運用自動化
        - k8sによるオートヒーリングやオートスケール。Custom Contoller構築による自動化
- 障害発生を前提とした設計
    - マイクロサービスは一部のサービスが失敗した場合でも、サービス全体を継続的に機能させる必要がある。このため、冗長性や自己修復機能を備えた設計が必要
- 進化的な設計
    - ある事例では当初モノリスで構築したが、新しい機能はマイクロサービスで構築し、既存のモノリスAPIを活用する
    - モノリスを徐々にマイクロサービス化するストラングラーパターンの考え方を理解しておく

## **[The Twelve Factors](https://12factor.net/ja/)**

- **設定を環境変数に格納する**

## **[Beyond the Twelve-Factor App](https://zenn.dev/kazurof/articles/18256f0e9c4761)**

## マイクロサービス間のデータ整合を取るためのパターン

- マイクロサービスごとにデータベースを分割するのが一般的
- ただし、あまりにマイクロサービスを細かく分割すると、実装が複雑になりバグの温床になる
- したがって業務的依存関係を見極めた上でのマイクロサービス分割することが大前提

## レガシーモダナイゼーション

- リホスト
    - OSやミドルウェアを刷新し、アプリを極力そのまま載せ替えることにより、アプリ再構築に関わることコストを極力かけずにシステムを更改する
- リファクタリング
- リライト
    - 技術者確保を目的として、OSやミドルウェアを刷新し、アプリケーションの仕様を変えず、Javaなどの別の言語で再構築
- リプレイス
    - アウトソーシングを目的とし、SaasやASPなどを用いて既存資産を置き換える
- リドキュメント
    - システムの可視化を目的とし、リエンジニアリング手法等を用いて古くなったドキュメントを再構築
- リインターフェース
    - 新規ビジネスへの迅速な対応を目的とし、OS、ミドルウェア、アプリまで既存資産を極力生かし、API技術などを用いて、フロント画面部分のみ刷新する。
- リビルド
    - 保守性の向上、OS,ミドルウェアの保守期限切れ、新規ビジネスの対応、技術者の確保を目的として、既存資産を全て再構築。
- リタイア
    - 公開不要な業務やアプリケーションを削除
- リテイン
    - 移行せず、元の環境で稼働させ続ける

## ****Autify****

- ノーコードで誰でも簡単、AIが自動でメンテナンス
- テスト自動化 で 高速・高品質 なリリースを実現

## Mabl

- E2E テストをお手軽にメンテできるツール
## Argo Rollouts

- kubernets向けのプログレッシブデリバリを提供する
    - Blue-Greenデプロイ
    - カナリアデプロイ
- Argo Rolloutsは以下のトラフィック制御ツールに対応している
    - Istio
    - nginx
    - AWS App Mesh
    - AWS ALBなど
- そのためIstioベースのカナリアデプロイができる
- Argo Rolloutsでは、RolloutsリソースがKubernetes標準のDeploymentリソースを置き換える形を利用し、デリバリ戦略を定義する
- デプロイしたアプリケーションに不具合が発生した場合、ロールバックできる

```yaml
strategy:
    canary:
      maxSurge: "25%"
      maxUnavailable: 0
      analysis:
        templates:
        - templateName: success-rate
        args:
        - name: service-name
          value: service-canary.default.svc.cluster.local
      canaryService: service-canary # 新しくデプロイするサービス
      stableService: service-stable # デプロイ済みのサービス
      trafficRouting:
        istio:
          virtualService:
            name: vsvc
      steps: # カナリアデプロイの実行ステップを順番に記載
      - setWeight: 20 # 20%
      - pause: {duration: 60} # 60秒待機
      - setWeight: 50
      - pause: {duration: 60}
      - setWeight: 80
      - pause: {duration: 60}
      - setWeight: 100
```
